LOG_DIR = "./sb_test/"  # путь к папке, в которой будет сохранен лог эксперимента
LEARNING_RATE = 3e-4  # скорость обучения
N_STEPS = 10  # количество действий совершаемых в каждой среде для накопления опыта перед обновлением весов
TOTAL_TIMESTEPS = 20  # суммарное количество действий, которое бедет совершено за весь период обучения
BATCH_SIZE = 4  # размер батча
N_EPOCHS = 2  # количество PPO-эпох (итераций обновления весов с набранным опытом)
GAMMA = 0.99  # коэффициент дисконтирования награды
GAE_LAMBDA = 0.95  # коэффициент лямбда в формуле advantage'а
CLIP_RANGE = 0.2  # параметр clip range алгоритма ppo для стратегии
CLIP_RANGE_VF = None  # параметр clip range алгоритма ppo для функции ценности
ENT_COEF = 0.001  # коэффициент штрафа за низкую энтропию
VF_COEF = 0.5  # коэффициент лоса для функции ценности
MAX_GRAD_NORM = 0.5  # максимальная норма градиента
VERBOSE = 1  # писать ли что-то в консоль
DEVICE = "auto"  # девайс для бэкпропа
MA_WINDOW = 10  # ширина окна скользящего среднего для черчения графика награды
